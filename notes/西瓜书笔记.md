# 西瓜书笔记

[toc]



## 第一章 绪论

### 引言

### 基本术语

1. 数据集（Dataset）：样本的集合
2. 样本（Sample）：**每个对象都是一个样本**
3. 维度（Dimension）：**在二维数据表中，一个列可以看成一个维度。** ***维度是对特征的计数***
4. 特征（Feature）：特征是描述样本的属性或观察值。可以将特征视为样本在特征空间中的维度。例如，对于一张图像样本，其特征可以是像素的亮度值或颜色分布。选择合适的特征对于机器学习模型的性能至关重要。
5. 模型（Model）：模型是机器学习算法生成的表示数据关系的**函数或数学表达式**。模型通过从训练数据中学习，然后可以用于预测新样本的输出或进行其他任务。常见的机器学习模型包括线性回归、决策树、神经网络等。
6. 训练（Training）：
7. 测试（Testing）：
8. 算法（Algorithm）：
9. 评估函数（Evaluation Metric）：评估函数用于衡量机器学习模型的性能和准确度。它们可以根据任务的不同而变化，例如精度（Accuracy）、均方误差（Mean Squared Error）、召回率（Recall）、精确率（Precision）等。选择合适的评估函数可以帮助我们判断模型的优劣。

### 假设空间

> 指由一个算法或模型可以表示的所有可能解或函数的集合。假设空间定义了模型可以学习和表达的所有可能的假设。

### 归纳偏好

> 习算法对假设空间中的某些假设更有倾向性或偏好的一种倾向。它是算法设计者为了提高学习算法的效果而对模型、算法或搜索过程进行的一种设定。
>
> **奥卡姆剃刀**

NFL(No  Free Lunch Theorem) 定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛的谈论”什么学习算法更好“毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好。

## 第二章 模型评估与选择

### 经验误差与过拟合

**经验误差/(训练误差)** : 与之相对应的是**泛化误差**(开放世界的模型误差)

**过拟合** : 模型在训练数据上表现良好，但在未见过的新数据上表现较差的现象，具体原因是因为模型的拟合效果过强，将数据集中的噪声也一并学习了 

### 评估方法

> 通过实验测试来对学习器的**泛化误差**进行评估

#### 留出法

##### 概念

> 将原始数据集分割成两个互斥的子集：训练集和测试集。
>
> ![图](http://image.zzzsleep.icu/fb2f020a34d5bc5b.png)

==需要注意的是，训练集和测试集的划分要尽可能的保持数据分布的一致性== 

##### 存在问题

> 1. 数据集的划分可能会对模型的评估结果产生一定的波动，因为不同的划分方式可能得到不同的评估结果。为了解决这个问题，一种常见的做法是执行多次随机划分，然后对评估结果进行平均。
> 2. 对于较小的数据集，使用留出法会导致训练样本的减少，可能会影响模型的性能。

##### 适用情况

> 适用于**数据集较大**、计算资源有限的情况下。它对于**初步评估模型的性能和优化模型参数**是很有用的。同时，在快速原型设计和模型迭代过程中，留出法可以提供初步反馈，帮助指导后续工作。

---

#### 交叉验证法（目前比较主流的模型评估方法）

##### 概念

> 它通过将原始数据集划分为多个互斥的子集，然后使用这些子集进行模型的训练和评估。其中最常用的方法是K折交叉验证。（K折交叉验证将数据集划分为K个近似大小的折（folds），每个折都被轮**流用作测试集**，剩下的K-1个折作为训练集。）
>
> **当数据集较小**，无法分出独立的验证集时，**交叉验证尤为有用**。它可以最大程度地利用数据，提供合理的模型评估结果。
>
> ![img](http://image.zzzsleep.icu/v2-ff995d9c6b5be17ec55153ee78fd8a40_b.jpg)

##### 存在的问题

> 在数据集较大时，训练m个模型的计算开销可能是难以忍受的
>
> 另外留一法的估计结果也未必比其他评估模型准确

##### 适用范围

> 和留出法互补，适用数据集较小时

---

#### 自助法

##### 概念

> 有放回的采样
>
> 从原始数据中通过自助采样得到的数据集会存在，有一部分样本多次出现，有一部分样本始终不出现
>
> 通过估计求极限可知，有0.368的数据始终不会出现
>
> ![总结：Bootstrap(自助法)，Bagging，Boosting(提升)](http://image.zzzsleep.icu/frc-fc3d9a34e09acb6f0d7895ea979b730f.png)

##### 存在的问题

> 改变了数据的原始分布，会引入估计误差

##### 适用范围

> 1. 数据集较小，难以有效划分训练/测试集时很有效
> 2. 自助法能从初始数据集中产生多个不同的训练集，这对集成学习有很大的好处

### 性能度量

> 对学习器的泛化性能评估，不仅需要有效可行的实验估计方法，还需要 **衡量模型泛化能力的评价标准** ，也就是性能度量

**==回归任务==** 最常用的性能度量是 **均方误差** (MSE)

> ![image-20230724173819984](http://image.zzzsleep.icu/image-20230724173819984.png)
>
> ![image-20230724173833910](http://image.zzzsleep.icu/image-20230724173833910.png) 

*下面的性能度量是对分类问题使用的*

#### 错误率和精度

> 错误率：分类错误的样本数，占样本总数的比例
>
> 精度：分类正确的样本数占样本总数的比例

==这两个是互斥的，精度=1-错误率==  

#### 查准率、查全率和F1

> 查准率（Precision）：模型预测为正类的样本中，真正为正类的比例
>
> 查全率（Recall）：模型正确预测为正类的样本在真实正类样本中的比例
>
> ![img](http://image.zzzsleep.icu/v2-fffc29aa2c160178fdf5f1f3a1ca1af6_b.jpg)
>
> **F1** ：F1是基于查准率和查全率的调和均值定义的
>
> ![img](http://image.zzzsleep.icu/v2-e91ff86b717af8c00d2fd5b677989785_b.jpg)



**调和均值：** 将各个指标的倒数取平均，再取倒数得到的结果

调和均值的特点之一是对较小的数值更为敏感。当数据集中存在较小的数时，它们的倒数将变大，因此在求和过程中它们会对结果产生更大的影响。换句话说，较小的数值对调和均值的贡献更大。

==使用调和均值来定义F1==

1. **平衡性**：F1分数通过调和均值将查全率和查准率进行结合，平衡了这两个指标之间的权衡关系。**当查全率和查准率相同时，F1分数达到最大值。**这意味着，在F1分数上取得较高的值，既需要较高的查全率，又需要较高的查准率。

2. 对不平衡数据集更敏感：**在处理不平衡数据集时，使用F1分数可以提供更可靠的性能评估**。对于正例和负例样本数量差异较大的情况，仅使用查全率或查准率可能会导致偏见。而F1分数综合考虑了这两个指标，更好地反映了模型在不平衡数据集上的整体效果。

   **对不平衡的数据集更敏感的意思：** 

   > 假设我们有一个拥有1000个样本的二分类问题，其中只有1个样本是正例，而剩下的999个样本都是负例。
   >
   > 查准率 1
   >
   > 查全率  0.001
   >
   > F1： 0.001998
   >
   > 假设我们有一个拥有1000个样本的二分类问题，其中只有1个样本是负例，而剩下的999个样本都是正例。
   >
   > 查准率：0.001
   >
   > 查全率：1
   >
   > F1： 0.001998
   >
   > **对查全率和查准率的关注度相同**

   但是我们并不是始终对这两个指标的关注的相同

   ![img](http://image.zzzsleep.icu/20210117161202538.png)

#### ROC 和 AUC

> ROC曲线：理想情况下，ROC曲线应该尽可能地靠近左上角，表示较高的真正例率和较低的假正例率，同时也意味着模型具有较高的性能。**ROC曲线下方的面积**，也称为AUC（Area Under the Curve），是评估模型性能的指标，AUC值越接近1，表示模型性能越好。
>
> AUC：一个完美的分类器的AUC为1，表示在所有分类阈值下，模型的真正例率始终大于假正例率，即模型的预测结果非常准确。一个随机预测的模型的AUC约为0.5，表示模型的预测能力与随机猜测相当。
>
> ![1_pk05QGzoWhCgRiiFbz-oKQ](http://image.zzzsleep.icu/1_pk05QGzoWhCgRiiFbz-oKQ.webp)

#### 代价敏感错误率和代价曲线

> 为了权衡不同类型的错误所造成的不同的损失，可为错误赋予”非均等代价“
>
> ![image-20230724203806359](http://image.zzzsleep.icu/image-20230724203806359.png)

![img](http://image.zzzsleep.icu/20200518195749878.png) 

### 比较检验

> 比较检验是一种统计学方法，用于比较两个或多个样本之间的差异，并通过计算得出是否存在显著性差异的结论。
>
> *但是在实践中很少使用该方法*

#### 假设检验

> 假设检验是一种统计学方法，通过收集样本数据并**计算与原假设相矛盾程度的统计量来判断是否拒绝原假设**，以得出对总体参数或总体之间关系的推断和决策。
>
> ![image-20230724204557676](http://image.zzzsleep.icu/image-20230724204557676.png)

#### 交叉验证 t 检验

> 交叉验证t检验是一种在比较不同机器学习模型性能时结合了交叉验证和t检验的统计方法，用于可靠地评估模型的差异并做出决策。
>
> **举例入下**
>
> ![image-20230724223555186](http://image.zzzsleep.icu/image-20230724223555186.png)
>
> **t检验**：t检验是一种用于比较两组样本均值是否存在显著性差异的统计方法。
>
> **举例如下**
>
> ![image-20230724235022489](http://image.zzzsleep.icu/image-20230724235022489.png)

#### McNemar 检验

> McNemar检验是一种用于比较两个相关样本的分类结果是否存在显著性差异的统计方法，通常应用于配对数据或重复测量实验设计中。它基于一个二元的列联表，通过比较错误分类的配对观测次数来检验差异是否显著。
>
> ![image-20230724235902078](http://image.zzzsleep.icu/image-20230724235902078.png)

#### Friedman 检验 和 Nemenyi 后续检验

> Friedman 检验：用于比较多个机器学习算法在多个数据集上的性能差异是否显著的统计检验方法。
>
> ![image-20230725001244406](http://image.zzzsleep.icu/image-20230725001244406.png)
>
> Nemenyi 后续检验：在Friedman检验显示存在显著差异时，用于确定哪些算法之间的性能差异是显著的多重比较方法，通过绘制Nemenyi图并比较线与临界值来得出结论。
>
> ![image-20230725001146881](http://image.zzzsleep.icu/image-20230725001146881.png)

### 偏差与方差



## 第三章 线性模型

### 基本形式

### 线性回归

### 对数几率回归

### 线性判别分析

### 多分类学习

### 类别不平衡问题

## 第五章 神经网络

### 神经元模型

### 感知机与多层神经网络

### 误差逆传播算法

### 全局最小与局部极小

### 其他常见神经网络

### 深度学习

## 第六章 支持向量机

### 间隔与支持向量

### 对偶函数

### 核函数

### 软间隔与正则化

### 支持向量回归

### 核方法

## 第七章 贝叶斯分类器

### 贝叶斯决策论

### 极大似然估计

### 朴素贝叶斯分类器

### 半朴素贝叶斯分类器

### 贝叶斯网

### EM算法

## 第八章 集成学习

### 个体与集成

### Boosting

### Bagging与随机森林

### 结合策略

### 多样性

## 第九章 聚类

### 聚类任务

### 性能度量

### 距离计算

### 原型聚类

### 密度聚类

### 层次聚类

## 第十章 降维与度量学习

### k近邻学习

### 低维嵌入

### 主成分分析

### 核化线性降维

### 流形学习

### 度量学习

## 第十一章 特征选择与稀疏学习

## 第十二章 计算学习理论

## 第十三章 半监督学习

## 第十四章 概率图模型

## 第十六章 强化学习

